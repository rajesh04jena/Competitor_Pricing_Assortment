{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d332d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from datetime import date, datetime\n",
    "import numpy as np\n",
    "import random\n",
    "import decimal\n",
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# Configuration and utility functions\n",
    "def setup_config():\n",
    "    \"\"\"Set up working directory and return configuration parameters\"\"\"\n",
    "    os.chdir(\"D:/projects/scraping_code_send_adhoc/\")\n",
    "    return 1  # total_pages\n",
    "\n",
    "def load_amazon_meta_urls():\n",
    "    \"\"\"Load Amazon meta URLs from CSV file\"\"\"\n",
    "    return pd.read_csv('amazon_meta_urls.csv', sep=',')\n",
    "\n",
    "def init_browser():\n",
    "    \"\"\"Initialize and return a configured Chrome browser instance\"\"\"\n",
    "    options = ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--incognito\")\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    return Chrome(options=options)\n",
    "\n",
    "def delete_cache(driver):\n",
    "    \"\"\"Clear browser cache\"\"\"\n",
    "    driver.execute_script(\"window.open('')\")\n",
    "    driver.switch_to.window(driver.window_handles[-1])\n",
    "    driver.get('chrome://settings/clearBrowserData')\n",
    "    perform_actions(driver, Keys.TAB * 2 + Keys.DOWN * 4 + Keys.TAB * 5 + Keys.ENTER)\n",
    "    driver.close()\n",
    "    driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "def perform_actions(driver, keys):\n",
    "    \"\"\"Perform keyboard actions\"\"\"\n",
    "    actions = ActionChains(driver)\n",
    "    actions.send_keys(keys)\n",
    "    time.sleep(1.5)\n",
    "    actions.perform()\n",
    "\n",
    "# ASIN Scraping functions\n",
    "def scrape_asin_urls(amazon_listing_url, total_pages):\n",
    "    \"\"\"Main function to scrape ASINs from listing pages\"\"\"\n",
    "    final_collated_urls = pd.DataFrame()\n",
    "    \n",
    "    for base_url in amazon_listing_url['landing page url'].unique().tolist():\n",
    "        all_product_urls = process_base_url(base_url, total_pages)\n",
    "        final_collated_urls = pd.concat([final_collated_urls, all_product_urls])\n",
    "        final_collated_urls = final_collated_urls.drop_duplicates('asin')\n",
    "    \n",
    "    final_collated_urls['Product URL'] = 'https://www.amazon.in/dp/' + final_collated_urls['asin'].astype(str)\n",
    "    final_collated_urls = pd.merge(final_collated_urls, amazon_listing_url, \n",
    "                                 left_on=['listing_url'], right_on=['landing page url'], how='left')\n",
    "    return final_collated_urls\n",
    "\n",
    "def process_base_url(base_url, total_pages):\n",
    "    \"\"\"Process all pages for a single base URL\"\"\"\n",
    "    all_product_urls = pd.DataFrame()\n",
    "    \n",
    "    for page in range(1, total_pages + 1):\n",
    "        url = f\"{base_url}&page={page}\"\n",
    "        browser = init_browser()\n",
    "        browser.get(url)\n",
    "        html = browser.page_source\n",
    "        delete_cache(browser)\n",
    "        browser.quit()\n",
    "        time.sleep(random.uniform(0.5, 1.05))\n",
    "        \n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        page_asin_df = extract_asin_from_page(soup)\n",
    "        page_asin_df['listing_url'] = base_url\n",
    "        all_product_urls = pd.concat([all_product_urls, page_asin_df])\n",
    "    \n",
    "    return all_product_urls.reset_index(drop=True)\n",
    "\n",
    "def extract_asin_from_page(soup):\n",
    "    \"\"\"Extract ASINs from a single page\"\"\"\n",
    "    product_urls = []\n",
    "    for div in soup.find_all(\"div\"):\n",
    "        if (data_asin := div.get(\"data-asin\")) and (data_index := div.get(\"data-index\")):\n",
    "            product_urls.append({'asin': data_asin, 'product default order number': data_index})\n",
    "    return pd.DataFrame(product_urls)\n",
    "\n",
    "# Product Detail Scraping functions\n",
    "def scrape_product_details(final_collated_urls):\n",
    "    \"\"\"Main function to scrape product details from product pages\"\"\"\n",
    "    final_scrapped_df = pd.DataFrame()\n",
    "    \n",
    "    for product_url in final_collated_urls['Product URL'].tolist():\n",
    "        product_df = scrape_single_product(product_url)\n",
    "        product_df['Category'] = final_collated_urls.loc[\n",
    "            final_collated_urls['Product URL'] == product_url, 'Category'].values[0]\n",
    "        product_df['Retailer'] = 'Amazon'\n",
    "        final_scrapped_df = pd.concat([final_scrapped_df, product_df])\n",
    "    \n",
    "    return final_scrapped_df\n",
    "\n",
    "def scrape_single_product(product_url):\n",
    "    \"\"\"Scrape details from a single product page\"\"\"\n",
    "    product_df = pd.DataFrame({'Product URL': [product_url]})\n",
    "    try:\n",
    "        browser = init_browser()\n",
    "        browser.get(product_url)\n",
    "        html = browser.page_source\n",
    "        delete_cache(browser)\n",
    "        browser.quit()\n",
    "        time.sleep(random.uniform(0.5, 1.05))\n",
    "        \n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        product_df = extract_basic_info(soup, product_df)\n",
    "        product_df = extract_pricing_info(soup, product_df)\n",
    "        product_df = extract_ratings_info(soup, product_df)\n",
    "        product_df = extract_additional_info(soup, product_df)\n",
    "        product_df = extract_technical_details(soup, product_df)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {product_url}: {str(e)}\")\n",
    "    \n",
    "    return product_df\n",
    "\n",
    "def extract_basic_info(soup, df):\n",
    "    \"\"\"Extract basic product information\"\"\"\n",
    "    try: df['Title'] = soup.find(\"span\", {'id': 'productTitle'}).text.strip()\n",
    "    except: df['Title'] = ''\n",
    "    \n",
    "    try: df['SKU Product'] = soup.find('input', {'id': 'ASIN'}).get('value')\n",
    "    except: df['SKU Product'] = ''\n",
    "    \n",
    "    df['Scraping Date'] = date.today()\n",
    "    df['Scraping Time'] = datetime.now()\n",
    "    return df\n",
    "\n",
    "def extract_pricing_info(soup, df):\n",
    "    \"\"\"Extract pricing-related information\"\"\"\n",
    "    try:\n",
    "        price = soup.find('span', {'class': 'a-price-whole'}).text.strip()\n",
    "        df['Selling Price'] = float(re.sub(r'[^\\d.]', '', price))\n",
    "    except: df['Selling Price'] = ''\n",
    "    \n",
    "    try:\n",
    "        mrp = soup.find(\"span\", class_=\"a-price a-text-price\").find(\"span\", class_=\"a-offscreen\").text\n",
    "        df['MRP'] = float(re.sub(r'[^\\d.]', '', mrp))\n",
    "    except: df['MRP'] = ''\n",
    "    \n",
    "    try: df['Discount'] = soup.find(\"span\", class_=\"a-size-large a-color-price\").text.strip().replace(\"-\", \"\")\n",
    "    except: df['Discount'] = ''\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_ratings_info(soup, df):\n",
    "    \"\"\"Extract rating-related information\"\"\"\n",
    "    try: df['no_ratings'] = soup.find(\"span\", {'id': 'acrCustomerReviewText'}).text.strip()\n",
    "    except: df['no_ratings'] = ''\n",
    "    \n",
    "    try: df['avg_rating'] = float(soup.find(\"span\", {'class': 'reviewCountTextLinkedHistogram'}).text.split()[0])\n",
    "    except: df['avg_rating'] = ''\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_additional_info(soup, df):\n",
    "    \"\"\"Extract additional product information\"\"\"\n",
    "    try: df['Stock Status'] = soup.find(\"div\", {'id': 'availabilityInsideBuyBox_feature_div'}).text.strip()\n",
    "    except: df['Stock Status'] = ''\n",
    "    \n",
    "    try: df['Seller'] = soup.find(\"a\", {'id': 'sellerProfileTriggerId'}).text.strip()\n",
    "    except: df['Seller'] = ''\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_technical_details(soup, df):\n",
    "    \"\"\"Extract technical details from product tables\"\"\"\n",
    "    tables = [\n",
    "        ('productOverview_feature_div', 'a-normal a-spacing-micro'),\n",
    "        ('productDetails_detailBullets_sections1', None),\n",
    "        ('productDetails_techSpec_section_1', None)\n",
    "    ]\n",
    "    \n",
    "    for table_id, table_class in tables:\n",
    "        try:\n",
    "            table = soup.find('div', {'id': table_id}).find('table', class_=table_class)\n",
    "            temp_df = pd.DataFrame([\n",
    "                [th.text.strip(), td.text.strip()] \n",
    "                for tr in table.find_all('tr') \n",
    "                for th, td in zip(tr.find_all('th'), tr.find_all('td'))\n",
    "            ], columns=['Attribute_Name', 'Attribute_Value'])\n",
    "            \n",
    "            df = pd.concat([df, temp_df], axis=1).ffill()\n",
    "        except: continue\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Main execution flow\n",
    "def main():\n",
    "    total_pages = setup_config()\n",
    "    amazon_listing_url = load_amazon_meta_urls()\n",
    "    \n",
    "    # Define unused but original variables\n",
    "    amazon_urls = {'https://www.amazon.in/s?rh=n%3A3474656031&fs=true', ...}  # shortened for brevity\n",
    "    category = {'AC', 'Air Purifier', ...}  # shortened for brevity\n",
    "    \n",
    "    # ASIN Scraping\n",
    "    final_collated_urls = scrape_asin_urls(amazon_listing_url, total_pages)\n",
    "    final_collated_urls.to_csv('product_urls_amazon.csv', index=False)\n",
    "    \n",
    "    # Product Detail Scraping\n",
    "    final_scrapped_df = scrape_product_details(final_collated_urls)\n",
    "    final_scrapped_df.to_csv('amazon_scrapped_data.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
