{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcd302a7-7e11-4e82-97d9-38140e96711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from datetime import date, datetime\n",
    "import numpy as np\n",
    "import random\n",
    "import decimal\n",
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d332d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_browser():\n",
    "    \"\"\"Initialize and return a configured Chrome browser instance\"\"\"\n",
    "    options = ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--incognito\")\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    return Chrome(options=options)\n",
    "\n",
    "def delete_cache(driver):\n",
    "    driver.execute_script(\"window.open('')\")  # Create a separate tab than the main one\n",
    "    driver.switch_to.window(driver.window_handles[-1])  # Switch window to the second tab\n",
    "    #driver.get('chrome://settings/clearBrowserData')  # Open your chrome settings.\n",
    "    #driver.findElement(By.xpath(\"//*[@id='clearBrowsingDataConfirm']\")).click()\n",
    "    driver.execute_cdp_cmd(\"Network.clearBrowserCache\", {})\n",
    "   #perform_actions(driver, Keys.TAB * 2 + Keys.DOWN * 4 + Keys.TAB * 5 + Keys.ENTER)  # Tab to the time select and key down to say \"All Time\" then go to the Confirm button and press Enter\n",
    "    driver.close()  # Close that window\n",
    "    driver.switch_to.window(driver.window_handles[0])  # Switch Selenium controls to the original tab to continue normal functionality.\n",
    "\n",
    "def perform_actions(driver, keys):\n",
    "    actions = ActionChains(driver)\n",
    "    actions.send_keys(keys)\n",
    "    time.sleep(1.5)\n",
    "    print('Performing Actions!')\n",
    "    actions.perform()\n",
    "\n",
    "# ASIN Scraping functions\n",
    "def scrape_asin_urls(amazon_listing_url, total_pages):\n",
    "    \"\"\"Main function to scrape ASINs from listing pages\"\"\"\n",
    "    final_collated_urls = pd.DataFrame()\n",
    "    \n",
    "    for base_url in amazon_listing_url:\n",
    "        all_product_urls = process_base_url(base_url, total_pages)\n",
    "        final_collated_urls = pd.concat([final_collated_urls, all_product_urls])\n",
    "        final_collated_urls = final_collated_urls.drop_duplicates('asin')\n",
    "    \n",
    "    final_collated_urls['Product URL'] = 'https://www.amazon.in/dp/' + final_collated_urls['asin'].astype(str)\n",
    "    return final_collated_urls\n",
    "\n",
    "def process_base_url(base_url, total_pages):\n",
    "    \"\"\"Process all pages for a single base URL\"\"\"\n",
    "    all_product_urls = pd.DataFrame()\n",
    "    \n",
    "    for page in range(1, total_pages + 1):\n",
    "        url = f\"{base_url}&page={page}\"\n",
    "        browser = init_browser()\n",
    "        browser.get(url)\n",
    "        html = browser.page_source\n",
    "        delete_cache(browser)\n",
    "        browser.quit()\n",
    "        time.sleep(random.uniform(0.5, 1.05))\n",
    "        \n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        page_asin_df = extract_asin_from_page(soup)\n",
    "        page_asin_df['listing_url'] = base_url\n",
    "        all_product_urls = pd.concat([all_product_urls, page_asin_df])\n",
    "    \n",
    "    return all_product_urls.reset_index(drop=True)\n",
    "\n",
    "def extract_asin_from_page(soup):\n",
    "    \"\"\"Extract ASINs from a single page\"\"\"\n",
    "    product_urls = []\n",
    "    for div in soup.find_all(\"div\"):\n",
    "        if (data_asin := div.get(\"data-asin\")) and (data_index := div.get(\"data-index\")):\n",
    "            product_urls.append({'asin': data_asin, 'product default order number': data_index})\n",
    "    return pd.DataFrame(product_urls)\n",
    "\n",
    "# Product Detail Scraping functions\n",
    "def scrape_product_details(final_collated_urls):\n",
    "    \"\"\"Main function to scrape product details from product pages\"\"\"\n",
    "    final_scrapped_df = pd.DataFrame()\n",
    "    \n",
    "    for product_url in final_collated_urls['Product URL'].tolist():\n",
    "        product_df = scrape_single_product(product_url)\n",
    "        product_df['Category'] = final_collated_urls.loc[\n",
    "            final_collated_urls['Product URL'] == product_url, 'Category'].values[0]\n",
    "        product_df['Retailer'] = 'Amazon'\n",
    "        final_scrapped_df = pd.concat([final_scrapped_df, product_df])\n",
    "    \n",
    "    return final_scrapped_df\n",
    "\n",
    "def scrape_single_product(product_url):\n",
    "    \"\"\"Scrape details from a single product page\"\"\"\n",
    "    product_df = pd.DataFrame({'Product URL': [product_url]})\n",
    "    try:\n",
    "        browser = init_browser()\n",
    "        browser.get(product_url)\n",
    "        html = browser.page_source\n",
    "        delete_cache(browser)\n",
    "        browser.quit()\n",
    "        time.sleep(random.uniform(0.5, 1.05))\n",
    "        \n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        product_df = extract_basic_info(soup, product_df)\n",
    "        product_df = extract_pricing_info(soup, product_df)\n",
    "        product_df = extract_ratings_info(soup, product_df)\n",
    "        product_df = extract_additional_info(soup, product_df)\n",
    "        product_df = extract_technical_details(soup, product_df)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {product_url}: {str(e)}\")\n",
    "    \n",
    "    return product_df\n",
    "\n",
    "def extract_basic_info(soup, df):\n",
    "    \"\"\"Extract basic product information\"\"\"\n",
    "    try: df['Title'] = soup.find(\"span\", {'id': 'productTitle'}).text.strip()\n",
    "    except: df['Title'] = ''\n",
    "    \n",
    "    try: df['SKU Product'] = soup.find('input', {'id': 'ASIN'}).get('value')\n",
    "    except: df['SKU Product'] = ''\n",
    "    \n",
    "    df['Scraping Date'] = date.today()\n",
    "    df['Scraping Time'] = datetime.now()\n",
    "    return df\n",
    "\n",
    "def extract_pricing_info(soup, df):\n",
    "    \"\"\"Extract pricing-related information\"\"\"\n",
    "    try:\n",
    "        price = soup.find('span', {'class': 'a-price-whole'}).text.strip()\n",
    "        df['Selling Price'] = float(re.sub(r'[^\\d.]', '', price))\n",
    "    except: df['Selling Price'] = ''\n",
    "    \n",
    "    try:\n",
    "        mrp = soup.find(\"span\", class_=\"a-price a-text-price\").find(\"span\", class_=\"a-offscreen\").text\n",
    "        df['MRP'] = float(re.sub(r'[^\\d.]', '', mrp))\n",
    "    except: df['MRP'] = ''\n",
    "    \n",
    "    try: df['Discount'] = soup.find(\"span\", class_=\"a-size-large a-color-price\").text.strip().replace(\"-\", \"\")\n",
    "    except: df['Discount'] = ''\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_ratings_info(soup, df):\n",
    "    \"\"\"Extract rating-related information\"\"\"\n",
    "    try: df['no_ratings'] = soup.find(\"span\", {'id': 'acrCustomerReviewText'}).text.strip()\n",
    "    except: df['no_ratings'] = ''\n",
    "    \n",
    "    try: df['avg_rating'] = float(soup.find(\"span\", {'class': 'reviewCountTextLinkedHistogram'}).text.split()[0])\n",
    "    except: df['avg_rating'] = ''\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_additional_info(soup, df):\n",
    "    \"\"\"Extract additional product information\"\"\"\n",
    "    try: df['Stock Status'] = soup.find(\"div\", {'id': 'availabilityInsideBuyBox_feature_div'}).text.strip()\n",
    "    except: df['Stock Status'] = ''\n",
    "    \n",
    "    try: df['Seller'] = soup.find(\"a\", {'id': 'sellerProfileTriggerId'}).text.strip()\n",
    "    except: df['Seller'] = ''\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_technical_details(soup, df):\n",
    "    \"\"\"Extract technical details from product tables\"\"\"\n",
    "    tables = [\n",
    "        ('productOverview_feature_div', 'a-normal a-spacing-micro'),\n",
    "        ('productDetails_detailBullets_sections1', None),\n",
    "        ('productDetails_techSpec_section_1', None)\n",
    "    ]\n",
    "    \n",
    "    for table_id, table_class in tables:\n",
    "        try:\n",
    "            table = soup.find('div', {'id': table_id}).find('table', class_=table_class)\n",
    "            temp_df = pd.DataFrame([\n",
    "                [th.text.strip(), td.text.strip()] \n",
    "                for tr in table.find_all('tr') \n",
    "                for th, td in zip(tr.find_all('th'), tr.find_all('td'))\n",
    "            ], columns=['Attribute_Name', 'Attribute_Value'])\n",
    "            \n",
    "            df = pd.concat([df, temp_df], axis=1).ffill()\n",
    "        except: continue\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da022644-3a02-46a0-a411-455f584062ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pages = 2\n",
    "# Define unused but original variables\n",
    "\n",
    "# ASIN Scraping\n",
    "final_collated_urls = scrape_asin_urls(amazon_listing_url, total_pages)\n",
    "final_collated_urls.to_csv('product_urls_amazon.csv', index=False)\n",
    "\n",
    "# Product Detail Scraping\n",
    "final_scrapped_df = scrape_product_details(final_collated_urls)\n",
    "final_scrapped_df.to_csv('amazon_scrapped_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc0e9ec1-be35-4199-959e-ed3876242eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Air Conditioner</td>\n",
       "      <td>https://www.amazon.in/s?rh=n%3A3474656031&amp;fs=true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          category                                                url\n",
       "0  Air Conditioner  https://www.amazon.in/s?rh=n%3A3474656031&fs=true"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_listing_url = pd.DataFrame({ 'category' : ['Air Conditioner'] , 'url' : ['https://www.amazon.in/s?rh=n%3A3474656031&fs=true'] }) \n",
    "amazon_listing_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac56007c-9298-4058-a704-35ef374fe224",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentException",
     "evalue": "Message: invalid argument\n  (Session info: chrome=133.0.6943.142)\nStacktrace:\n0   chromedriver                        0x00000001049042d4 cxxbridge1$str$ptr + 2739836\n1   chromedriver                        0x00000001048fc934 cxxbridge1$str$ptr + 2708700\n2   chromedriver                        0x000000010445de04 cxxbridge1$string$len + 92964\n3   chromedriver                        0x0000000104447b08 cxxbridge1$string$len + 2088\n4   chromedriver                        0x0000000104445d74 chromedriver + 187764\n5   chromedriver                        0x00000001044468c0 chromedriver + 190656\n6   chromedriver                        0x0000000104460f40 cxxbridge1$string$len + 105568\n7   chromedriver                        0x00000001044e69b4 cxxbridge1$string$len + 653012\n8   chromedriver                        0x00000001044e5e80 cxxbridge1$string$len + 650144\n9   chromedriver                        0x0000000104499060 cxxbridge1$string$len + 335232\n10  chromedriver                        0x00000001048ccc38 cxxbridge1$str$ptr + 2512864\n11  chromedriver                        0x00000001048cff58 cxxbridge1$str$ptr + 2525952\n12  chromedriver                        0x00000001048b2578 cxxbridge1$str$ptr + 2404640\n13  chromedriver                        0x00000001048d0818 cxxbridge1$str$ptr + 2528192\n14  chromedriver                        0x00000001048a2f2c cxxbridge1$str$ptr + 2341588\n15  chromedriver                        0x00000001048eca60 cxxbridge1$str$ptr + 2643464\n16  chromedriver                        0x00000001048ecbe8 cxxbridge1$str$ptr + 2643856\n17  chromedriver                        0x00000001048fc5a8 cxxbridge1$str$ptr + 2707792\n18  libsystem_pthread.dylib             0x0000000180a17fa8 _pthread_start + 148\n19  libsystem_pthread.dylib             0x0000000180a12da0 thread_start + 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgumentException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m total_pages =\u001b[32m2\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m url_id \u001b[38;5;129;01min\u001b[39;00m amazon_listing_url[\u001b[33m'\u001b[39m\u001b[33murl\u001b[39m\u001b[33m'\u001b[39m].unique().tolist():    \n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     final_collated_urls = \u001b[43mscrape_asin_urls\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_pages\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mscrape_asin_urls\u001b[39m\u001b[34m(amazon_listing_url, total_pages)\u001b[39m\n\u001b[32m     31\u001b[39m final_collated_urls = pd.DataFrame()\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m base_url \u001b[38;5;129;01min\u001b[39;00m amazon_listing_url:\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     all_product_urls = \u001b[43mprocess_base_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_pages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     final_collated_urls = pd.concat([final_collated_urls, all_product_urls])\n\u001b[32m     36\u001b[39m     final_collated_urls = final_collated_urls.drop_duplicates(\u001b[33m'\u001b[39m\u001b[33masin\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mprocess_base_url\u001b[39m\u001b[34m(base_url, total_pages)\u001b[39m\n\u001b[32m     46\u001b[39m url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m&page=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     47\u001b[39m browser = init_browser()\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[43mbrowser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m html = browser.page_source\n\u001b[32m     50\u001b[39m delete_cache(browser)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/price-savant-test-env/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:454\u001b[39m, in \u001b[36mWebDriver.get\u001b[39m\u001b[34m(self, url)\u001b[39m\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    437\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Navigate the browser to the specified URL in the current window or\u001b[39;00m\n\u001b[32m    438\u001b[39m \u001b[33;03m    tab.\u001b[39;00m\n\u001b[32m    439\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    452\u001b[39m \u001b[33;03m    >>> driver.get(\"https://example.com\")\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43murl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/price-savant-test-env/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:429\u001b[39m, in \u001b[36mWebDriver.execute\u001b[39m\u001b[34m(self, driver_command, params)\u001b[39m\n\u001b[32m    427\u001b[39m response = \u001b[38;5;28mself\u001b[39m.command_executor.execute(driver_command, params)\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merror_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    430\u001b[39m     response[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._unwrap_value(response.get(\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    431\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/price-savant-test-env/lib/python3.12/site-packages/selenium/webdriver/remote/errorhandler.py:232\u001b[39m, in \u001b[36mErrorHandler.check_response\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m    230\u001b[39m         alert_text = value[\u001b[33m\"\u001b[39m\u001b[33malert\u001b[39m\u001b[33m\"\u001b[39m].get(\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    231\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[31mInvalidArgumentException\u001b[39m: Message: invalid argument\n  (Session info: chrome=133.0.6943.142)\nStacktrace:\n0   chromedriver                        0x00000001049042d4 cxxbridge1$str$ptr + 2739836\n1   chromedriver                        0x00000001048fc934 cxxbridge1$str$ptr + 2708700\n2   chromedriver                        0x000000010445de04 cxxbridge1$string$len + 92964\n3   chromedriver                        0x0000000104447b08 cxxbridge1$string$len + 2088\n4   chromedriver                        0x0000000104445d74 chromedriver + 187764\n5   chromedriver                        0x00000001044468c0 chromedriver + 190656\n6   chromedriver                        0x0000000104460f40 cxxbridge1$string$len + 105568\n7   chromedriver                        0x00000001044e69b4 cxxbridge1$string$len + 653012\n8   chromedriver                        0x00000001044e5e80 cxxbridge1$string$len + 650144\n9   chromedriver                        0x0000000104499060 cxxbridge1$string$len + 335232\n10  chromedriver                        0x00000001048ccc38 cxxbridge1$str$ptr + 2512864\n11  chromedriver                        0x00000001048cff58 cxxbridge1$str$ptr + 2525952\n12  chromedriver                        0x00000001048b2578 cxxbridge1$str$ptr + 2404640\n13  chromedriver                        0x00000001048d0818 cxxbridge1$str$ptr + 2528192\n14  chromedriver                        0x00000001048a2f2c cxxbridge1$str$ptr + 2341588\n15  chromedriver                        0x00000001048eca60 cxxbridge1$str$ptr + 2643464\n16  chromedriver                        0x00000001048ecbe8 cxxbridge1$str$ptr + 2643856\n17  chromedriver                        0x00000001048fc5a8 cxxbridge1$str$ptr + 2707792\n18  libsystem_pthread.dylib             0x0000000180a17fa8 _pthread_start + 148\n19  libsystem_pthread.dylib             0x0000000180a12da0 thread_start + 8\n"
     ]
    }
   ],
   "source": [
    "total_pages =2\n",
    "for url_id in amazon_listing_url['url'].unique().tolist():    \n",
    "    final_collated_urls = scrape_asin_urls(url_id, total_pages)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "216f8012-beb6-43e2-9ece-2125c60fa4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Delayed('scrape_asin_urls-b9500aa0-9a6b-48f4-b67f-db89b3bc82ac'),\n",
       "  'https://www.amazon.in/s?rh=n%3A3474656031&fs=true')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_collated_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d59eed-33f7-4aef-8a68-38539ac89c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pages = 1\n",
    "# Define unused but original variables\n",
    "\n",
    "# ASIN Scraping\n",
    "final_collated_urls = scrape_asin_urls(amazon_listing_url, total_pages)\n",
    "final_collated_urls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
